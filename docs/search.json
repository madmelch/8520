[
  {
    "objectID": "presentation.html#intro",
    "href": "presentation.html#intro",
    "title": "Triangulating Resource Conservation Movements in Digital Spaces: Topic Modeling and Rhetorical Constellations on r/Anticonsumption",
    "section": "Intro",
    "text": "Intro\n\nDemocratization of climate change (CC) expertise\n\nEveryone is responding to CC in their own ways. Who is doing so explicitly in community with others, and how? What relationships to environment/resources define their community?"
  },
  {
    "objectID": "presentation.html#questions",
    "href": "presentation.html#questions",
    "title": "Triangulating Resource Conservation Movements in Digital Spaces: Topic Modeling and Rhetorical Constellations on r/Anticonsumption",
    "section": "Questions",
    "text": "Questions\n\nHow does r/Anticonsumption embody the democratization of CC expertise/expertise in responding to CC? How do users position themselves in relation to other people, resources, and environment at large?\n… too much. This became…\nWhat is the anti-consumption movement? What habituated practices and beliefs make r/Anticonsumption a community?\n\nAppropriate to comp methods\nReasonable for my skill set"
  },
  {
    "objectID": "presentation.html#methods",
    "href": "presentation.html#methods",
    "title": "Triangulating Resource Conservation Movements in Digital Spaces: Topic Modeling and Rhetorical Constellations on r/Anticonsumption",
    "section": "Methods",
    "text": "Methods\n\nSTM\n\nJames Cook (2023), Julia Silge (2018), Dan Card (2023)\n\nRhetorical analysis - ideological constellation mapping\n\nTillery & Bloomfield (2022)"
  },
  {
    "objectID": "presentation.html#topic-modeling",
    "href": "presentation.html#topic-modeling",
    "title": "Triangulating Resource Conservation Movements in Digital Spaces: Topic Modeling and Rhetorical Constellations on r/Anticonsumption",
    "section": "Topic Modeling",
    "text": "Topic Modeling\n\nr/Anticonsumption thread-starting posts: 971 objects, 7 variables\nCould not get Julia Silge’s multiprocessing to work, so I repeated this:\n{'''{{r}} # model K5 model_K5 &lt;- stm(documents = dfm_stm$documents,                  vocab = dfm_stm$vocab,                  K = 10,                  verbose = TRUE) plot(model_K5) summary(model_K5)"
  },
  {
    "objectID": "presentation.html#topic-modeling-1",
    "href": "presentation.html#topic-modeling-1",
    "title": "Triangulating Resource Conservation Movements in Digital Spaces: Topic Modeling and Rhetorical Constellations on r/Anticonsumption",
    "section": "Topic Modeling",
    "text": "Topic Modeling\n\n\nTopic 4 Top Words:\n  Highest Prob: t, just, like, buy, things, can, people \n  FREX: don, t, christmas, m, presents, thrifting, gifts \n  Lift: havana, loving, o, toy, #2, @adriftincyberspace, 0-8 \n  Score: t, don, christmas, gifts, halloween, m, havana"
  },
  {
    "objectID": "presentation.html#topic-modeling-2",
    "href": "presentation.html#topic-modeling-2",
    "title": "Triangulating Resource Conservation Movements in Digital Spaces: Topic Modeling and Rhetorical Constellations on r/Anticonsumption",
    "section": "Topic Modeling",
    "text": "Topic Modeling\n\nLanded on K30"
  },
  {
    "objectID": "presentation.html#topic-modeling-3",
    "href": "presentation.html#topic-modeling-3",
    "title": "Triangulating Resource Conservation Movements in Digital Spaces: Topic Modeling and Rhetorical Constellations on r/Anticonsumption",
    "section": "Topic Modeling",
    "text": "Topic Modeling\n\nFREX check K30\nTopic 15 Top Words:\n  Highest Prob: right, soil, can, people, now, individual, growth \n  FREX: organisms, soil, cell, biological, imperative, civilisation, super-organism \n  Lift: civilisation, super-organism, acceptable, accessory, adherence, adoption, alll \n  Score: organisms, biological, imperative, soil, reproduce, cell, growth \nTopic 16 Top Words:\n  Highest Prob: use, like, one, things, time, can, feel \n  FREX: states, tiny, shops, skills, common, sorting, whereas \n  Lift: 24th, acrobat, admitted, alteration, answering, barren, baubles \n  Score: whereas, states, tiny, cosy, there’d, default, celebrate"
  },
  {
    "objectID": "presentation.html#rhetorical-analysis---constellating",
    "href": "presentation.html#rhetorical-analysis---constellating",
    "title": "Triangulating Resource Conservation Movements in Digital Spaces: Topic Modeling and Rhetorical Constellations on r/Anticonsumption",
    "section": "Rhetorical Analysis - Constellating",
    "text": "Rhetorical Analysis - Constellating\n\nAt a glance:\n\n“people” indicates othering attitude, “us vs them”\nhome as site of control: clearing clutter, gardening, childcare, DIY, clothing re/use, food\n\nClose reading of select posts:\n\n“Us vs them”:“so many people,” “other people,” “why do people,” etc.\nGreen-ing personal life/space:“getting rid of,” “refurbishing,” “looking for ideas,” “recipes,” “holistic remedies,” “make [clothing/shoes] last longer”\n“Outside” world (not nature):criticizing corporations, delivery services, megastores, jobs/careers, big brands, corporate traditions, other people in user’s life, etc.\n\na third location where I don’t have to buy anything?”\nindustrialized world inhospitable to anti-consumers"
  },
  {
    "objectID": "presentation.html#proof-of-concept",
    "href": "presentation.html#proof-of-concept",
    "title": "Triangulating Resource Conservation Movements in Digital Spaces: Topic Modeling and Rhetorical Constellations on r/Anticonsumption",
    "section": "Proof of Concept?",
    "text": "Proof of Concept?\n\nMy conclusion: r/Anticonsumption users often position themselves in direct moral opposition to the industrialized world around them. They often seek/share advice about altering home space, home practices, and daily habits toward overall decreased consumption of goods.\nSubreddit bio: “/r/Anticonsumption is a sub primarily for criticizing and discussing consumer culture. This includes but is not limited to material consumption, the environment, media consumption, and corporate influence.”\nAn interested and pliable “yes” + at least two grains of salt"
  },
  {
    "objectID": "presentation.html#additional-uptakes",
    "href": "presentation.html#additional-uptakes",
    "title": "Triangulating Resource Conservation Movements in Digital Spaces: Topic Modeling and Rhetorical Constellations on r/Anticonsumption",
    "section": "Additional Uptakes",
    "text": "Additional Uptakes\n\nPower of asking the right question\nCommunity\nMy bigger picture: this offers signposts, practice, clarification"
  },
  {
    "objectID": "presentation.html#questions-1",
    "href": "presentation.html#questions-1",
    "title": "Triangulating Resource Conservation Movements in Digital Spaces: Topic Modeling and Rhetorical Constellations on r/Anticonsumption",
    "section": "Questions?",
    "text": "Questions?"
  },
  {
    "objectID": "experience report.html",
    "href": "experience report.html",
    "title": "Triangulating Resource Conservation Movements in Digital Spaces: Topic Modeling and Rhetorical Constellations on r/Anticonsumption",
    "section": "",
    "text": "In this project, I created a structured topic model (STM) of posts on r/Anticonsumption that revealed common threads of discussion, then performed rhetorical constellation analysis on and between the 30 topics. My goal was to define this movement based on its shared language practices. I found that r/Anticonsumption represents a community focused on diminishing consumerist tendencies, financial conservativism, and a feeling-based relationship to their external and internal environments."
  },
  {
    "objectID": "experience report.html#abstract",
    "href": "experience report.html#abstract",
    "title": "Triangulating Resource Conservation Movements in Digital Spaces: Topic Modeling and Rhetorical Constellations on r/Anticonsumption",
    "section": "",
    "text": "In this project, I created a structured topic model (STM) of posts on r/Anticonsumption that revealed common threads of discussion, then performed rhetorical constellation analysis on and between the 30 topics. My goal was to define this movement based on its shared language practices. I found that r/Anticonsumption represents a community focused on diminishing consumerist tendencies, financial conservativism, and a feeling-based relationship to their external and internal environments."
  },
  {
    "objectID": "experience report.html#introduction",
    "href": "experience report.html#introduction",
    "title": "Triangulating Resource Conservation Movements in Digital Spaces: Topic Modeling and Rhetorical Constellations on r/Anticonsumption",
    "section": "Introduction",
    "text": "Introduction\nWhat are users of the r/Anticonsumption space talking about? I want to learn about this community’s motivations, goals, joys, and anxieties, as well as in what climate contexts they see themselves situated (e.g., anthropocene, eremocene, state-guided versus corporate capitalism, neoliberalism, technological takeover). An additional component of this inquiry is defining the anticonsumption movement in general. The movement is relatively new and lacks a reliable definition (Lee 2022; Makri et al. 2020). STM and rhetorical constellation analysis can lend insight into the anticonsumption movement by better defining its character and goals, as well as addressing my question of how the community is situated in climate change discourse."
  },
  {
    "objectID": "experience report.html#methodology",
    "href": "experience report.html#methodology",
    "title": "Triangulating Resource Conservation Movements in Digital Spaces: Topic Modeling and Rhetorical Constellations on r/Anticonsumption",
    "section": "Methodology",
    "text": "Methodology\nI performed STM on threads posted to r/Anticonsumption within the past month. This yielded 971 items with 7 variables. To first scrape the necessary data from r/Anticonsumption and prepare the data for exploration, I used a YouTube tutorial from James Cook (2023).\nAfter scraping, I tested and trained topic models using a variety of K values ranging from K = 5 (K5) to K = 70 (K70). This included producing summaries of each topic in K = n (Kn) including the terms with highest probability to appear in posts within a given topic (“Highest Prob”) and the terms most frequent and exclusive to each topic (FREX). Code for this part of the process was provided by Dan Card (2023). The Kn with greatest commonality between topics’ Highest Prob and FREX terms was K30. Therefore, I elected to perform close reading and qualitative analysis on K30 topics. For this, I used code provided by Dan Card (2023) and Julia Silge (2018).\nFinally, my manual qualitative analysis was inspired by the rhetorical constellation method demonstrated by Tillery and Bloomfield (2022). Their definition of “prominences” within a given discourse as nodes of conversation that create a “constellation” inspired my coding practice for the close reading portion of my analysis. Tillery and Bloomfield also aim to discern the political ideologies underlying subsets of their data, and position various comments and therefore prominences somewhere on a “left” to “right” political spectrum. Since my aim was to definitionally describe r/Anticonsumption discourse, I did not pursue this ideologizing effort."
  },
  {
    "objectID": "experience report.html#my-code",
    "href": "experience report.html#my-code",
    "title": "Triangulating Resource Conservation Movements in Digital Spaces: Topic Modeling and Rhetorical Constellations on r/Anticonsumption",
    "section": "My Code",
    "text": "My Code"
  },
  {
    "objectID": "experience report.html#phase-1a-scraping-ranticonsumption---james-cook-2023",
    "href": "experience report.html#phase-1a-scraping-ranticonsumption---james-cook-2023",
    "title": "Triangulating Resource Conservation Movements in Digital Spaces: Topic Modeling and Rhetorical Constellations on r/Anticonsumption",
    "section": "Phase 1a: Scraping r/Anticonsumption - James Cook (2023)",
    "text": "Phase 1a: Scraping r/Anticonsumption - James Cook (2023)\n```{r}\n\n# get data\ninstall.packages(\"RedditExtractoR\")\ninstall.packages(\"kableExtra\")\nlibrary(RedditExtractoR)\nlibrary(kableExtra)\n\n# scrape r/Anticonsumption for threads\nac_threads &lt;- find_thread_urls(subreddit = \"Anticonsumption\", sort_by = \"new\")\n\nhead(ac_threads, 3) %&gt;%\n  kbl()\n\n\n```\n\nPhase 1a Inputs, Outputs, and Errors\nPhase 1a was the quickest and smoothest step in this research. I ran into no errors or sticking points with the code, and it worked just as well upon rerun. I gathered all thread-starting posts (sans comments) from the past month which included 971 objects of 7 variables, which looks about right."
  },
  {
    "objectID": "experience report.html#phase-1b-preparing-data---dan-card-2023",
    "href": "experience report.html#phase-1b-preparing-data---dan-card-2023",
    "title": "Triangulating Resource Conservation Movements in Digital Spaces: Topic Modeling and Rhetorical Constellations on r/Anticonsumption",
    "section": "Phase 1b: Preparing Data - Dan Card (2023)",
    "text": "Phase 1b: Preparing Data - Dan Card (2023)\n```{r}\n\n# create and read in CSV\nlibrary(dplyr)\n\nwrite.csv(ac_threads, \"data/ac_threads.csv\")\n\nread.csv(\"data/ac_threads.csv\")\n\n\n\n# clean data \nlibrary(janitor)\nlibrary(tidyverse)\n\nac_threads &lt;- clean_names(ac_threads)\n\nglimpse(ac_threads)\n\nac_threads &lt;- ac_threads %&gt;% \n  select(title,\n         text,\n         comments)\n\nglimpse(ac_threads)\n\n\n# create corpus\nlibrary(quanteda)\nlibrary(quanteda.dictionaries)\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textplots)\nlibrary(quanteda.textstats)\nlibrary(readtext)\nlibrary(spacyr)\nlibrary(tidyverse)\n\nthread_corp &lt;- corpus(ac_threads, text_field = \"text\")\n\n\n# tokenize the corpus\nthread_tokens &lt;- tokens(thread_corp, remove_punct = TRUE, remove_separators = TRUE, remove_numbers = TRUE)\n\nthread_tokens\n\n\n# remove stop words\nhead(stopwords(\"en\"), 50)\n\nthread_tokens_nostop &lt;- thread_tokens %&gt;% tokens_remove(stopwords(\"en\"))\n\ntidy_threads_dfm &lt;- dfm(thread_tokens_nostop)\n\n```\n\nPhase 1b Inputs, Outputs, and Errors\nPhase 1b went smoothly as well. The only errors that arose were from uncalled libraries or a missing package."
  },
  {
    "objectID": "experience report.html#phase-2-testing-and-training-models-to-find-kn---dan-card-2023",
    "href": "experience report.html#phase-2-testing-and-training-models-to-find-kn---dan-card-2023",
    "title": "Triangulating Resource Conservation Movements in Digital Spaces: Topic Modeling and Rhetorical Constellations on r/Anticonsumption",
    "section": "Phase 2: Testing and Training Models to Find Kn - Dan Card (2023)",
    "text": "Phase 2: Testing and Training Models to Find Kn - Dan Card (2023)\n```{r}\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(tidytext)\nlibrary(stm)\n\n\n# convert dfm to stm - Dan Card (2023)\nlibrary(stm)\ndfm_stm &lt;- convert(tidy_threads_dfm, to = \"stm\")\n\n\n# model K5\nmodel_K5 &lt;- stm(documents = dfm_stm$documents,\n             vocab = dfm_stm$vocab, \n             K = 10,\n             verbose = TRUE)\nplot(model_K5)\nsummary(model_K5)\n\n\n# repeat for K10\nmodel_K10 &lt;- stm(documents = dfm_stm$documents,\n                vocab = dfm_stm$vocab, \n                K = 10,\n                verbose = TRUE)\nplot(model_K10)\nsummary(model_K10)\n\n\n# K20\nmodel_K20 &lt;- stm(documents = dfm_stm$documents,\n                 vocab = dfm_stm$vocab, \n                 K = 20,\n                 verbose = TRUE)\nplot(model_K20)\nsummary(model_K20)\n\n\n# K30\nmodel_K30 &lt;- stm(documents = dfm_stm$documents,\n                 vocab = dfm_stm$vocab, \n                 K = 30,\n                 verbose = TRUE)\nplot(model_K30)\nsummary(model_K30)\n\n\n# K40\nmodel_K40 &lt;- stm(documents = dfm_stm$documents,\n                 vocab = dfm_stm$vocab, \n                 K = 40,\n                 verbose = TRUE)\nplot(model_K40)\nsummary(model_K40)\n\n\n# K50\nmodel_K50 &lt;- stm(documents = dfm_stm$documents,\n                 vocab = dfm_stm$vocab, \n                 K = 50,\n                 verbose = TRUE)\nplot(model_K50)\nsummary(model_K50)\n\n\n```\n\nPhase 2 Inputs, Outputs, and Errors\nI ran into a problem with running parallel tests for ideal K-values, i.e., Julia Silge’s code for model diagnostics by number of topics. I was unable to produce visuals showing held-out likelihood, lower bound, residuals, and semantic coherence for multiple K-values. To move forward, I simply re-ran a chunk of functional, well-understood code for each K-value I wished to explore as shown above.\nThe plot and summary functions following each iteration yielded visuals shown here (externally linked doc). Based on the commonalities between Highest Prob terms and FREX terms as well as researcher-observed semantic coherence of topics between all explored Kn, I elected to further explore K20 and K30."
  },
  {
    "objectID": "experience report.html#phase-3-modeling-k20-and-k30-to-determine-final-kn--julia-silge-2018",
    "href": "experience report.html#phase-3-modeling-k20-and-k30-to-determine-final-kn--julia-silge-2018",
    "title": "Triangulating Resource Conservation Movements in Digital Spaces: Topic Modeling and Rhetorical Constellations on r/Anticonsumption",
    "section": "Phase 3: Modeling K20 and K30 to Determine Final Kn- Julia Silge (2018)",
    "text": "Phase 3: Modeling K20 and K30 to Determine Final Kn- Julia Silge (2018)\n```{r}\n\n# exploring topics approaching ideal Kn\n# beta: highest word probabilities for each topic\n\n# K20\nk20_beta &lt;- tidy(model_K20)\n\nk20_beta %&gt;%\n  group_by(topic) %&gt;%\n  top_n(10, beta) %&gt;%\n  ungroup() %&gt;%\n  mutate(topic = paste0(\"Topic \", topic),\n         term = reorder_within(term, beta, topic)) %&gt;%\n  ggplot(aes(term, beta, fill = as.factor(topic))) +\n  geom_col(alpha = 0.8, show.legend = FALSE) +\n  facet_wrap(~ topic, scales = \"free_y\") +\n  coord_flip() +\n  scale_x_reordered() +\n  labs(x = NULL, y = expression(beta),\n       title = \"Highest word probabilities for each topic\",\n       subtitle = \" \")  \n\n# K30\nk30_beta &lt;- tidy(model_K30)\n\nk30_beta %&gt;%\n  group_by(topic) %&gt;%\n  top_n(10, beta) %&gt;%\n  ungroup() %&gt;%\n  mutate(topic = paste0(\"Topic \", topic),\n         term = reorder_within(term, beta, topic)) %&gt;%\n  ggplot(aes(term, beta, fill = as.factor(topic))) +\n  geom_col(alpha = 0.8, show.legend = FALSE) +\n  facet_wrap(~ topic, scales = \"free_y\") +\n  coord_flip() +\n  scale_x_reordered() +\n  labs(x = NULL, y = expression(beta),\n       title = \"Highest word probabilities for each topic\",\n       subtitle = \" \")  \n\n\n# gamma: probabilities that each doc is generated from each topic\n\n# K20\nk20_gamma &lt;- tidy(model_K20, matrix = \"gamma\",\n                  document_names = rownames(tidy_threads_dfm))\n\n# K30\nk30_gamma &lt;- tidy(model_K30, matrix = \"gamma\",\n                  document_names = rownames(tidy_threads_dfm))\n\n\n# visual for k20\ninstall.packages(\"scales\")\nlibrary(scales)\nlibrary(ggthemes)\n\n\ntop_terms_k20 &lt;- k20_beta %&gt;%\n  arrange(beta) %&gt;%\n  group_by(topic) %&gt;%\n  top_n(7, beta) %&gt;%\n  arrange(-beta) %&gt;%\n  select(topic, term) %&gt;%\n  summarise(terms = list(term)) %&gt;%\n  mutate(terms = map(terms, paste, collapse = \", \")) %&gt;% \n  unnest(cols = c(terms))\n\ngamma_terms_k20 &lt;- k20_gamma %&gt;%\n  group_by(topic) %&gt;%\n  summarise(gamma = mean(gamma)) %&gt;%\n  arrange(desc(gamma)) %&gt;%\n  left_join(top_terms, by = \"topic\") %&gt;%\n  mutate(topic = paste0(\"Topic \", topic),\n         topic = reorder(topic, gamma))\n\ngamma_terms_k20 %&gt;%\n  top_n(20, gamma) %&gt;%\n  ggplot(aes(topic, gamma, label = terms, fill = topic)) +\n  geom_col(show.legend = FALSE) +\n  geom_text(hjust = 0, nudge_y = 0.0005, size = 3,\n            family = \"IBMPlexSans\") +\n  coord_flip() +\n  scale_y_continuous(expand = c(0,0),\n                     limits = c(0, 0.09),\n                     labels = scales::label_percent()) +\n  theme_tufte(base_family = \"IBMPlexSans\", ticks = FALSE) +\n  theme(plot.title = element_text(size = 16,\n                                  family=\"IBMPlexSans-Bold\"),\n        plot.subtitle = element_text(size = 13)) +\n  labs(x = NULL, y = expression(gamma),\n       title = \"Top 20 topics by prevalence in r/Anticonsumption\",\n       subtitle = \" \")\n\n\n# visual for k30\ntop_terms_l30 &lt;- k30_beta %&gt;%\n  arrange(beta) %&gt;%\n  group_by(topic) %&gt;%\n  top_n(7, beta) %&gt;%\n  arrange(-beta) %&gt;%\n  select(topic, term) %&gt;%\n  summarise(terms = list(term)) %&gt;%\n  mutate(terms = map(terms, paste, collapse = \", \")) %&gt;% \n  unnest(cols = c(terms))\n\ngamma_terms_k30 &lt;- k30_gamma %&gt;%\n  group_by(topic) %&gt;%\n  summarise(gamma = mean(gamma)) %&gt;%\n  arrange(desc(gamma)) %&gt;%\n  left_join(top_terms, by = \"topic\") %&gt;%\n  mutate(topic = paste0(\"Topic \", topic),\n         topic = reorder(topic, gamma))\n\ngamma_terms_k30 %&gt;%\n  top_n(30, gamma) %&gt;%\n  ggplot(aes(topic, gamma, label = terms, fill = topic)) +\n  geom_col(show.legend = FALSE) +\n  geom_text(hjust = 0, nudge_y = 0.0005, size = 3,\n            family = \"IBMPlexSans\") +\n  coord_flip() +\n  scale_y_continuous(expand = c(0,0),\n                     limits = c(0, 0.09),\n                     labels = scales::label_percent()) +\n  theme_tufte(base_family = \"IBMPlexSans\", ticks = FALSE) +\n  theme(plot.title = element_text(size = 16,\n                                  family=\"IBMPlexSans-Bold\"),\n        plot.subtitle = element_text(size = 13)) +\n  labs(x = NULL, y = expression(gamma),\n       title = \"Top 30 topics by prevalence in r/Anticonsumption\",\n       subtitle = \" \")\n\n\n```\n\nPhase 3 Inputs, Outputs, and Errors\nThese plots list more top terms per topic than the previous ones and offer clearer visualization of the topic proportions. As I compared K20 and K30 using these visuals, it became clear that K30 offered greater semantic coherence within each topic as well as across groups of topics which featured overlapping top words. A key signifier of coherence was my personal response of follow-up question asking when observing K30 topics versus definitional confusion while looking through K20 topics.\nAfter this phase, I proceeded to analyze topics under K30 using manual qualitative methods."
  },
  {
    "objectID": "experience report.html#phase-4-rhetorical-constellating-on-k30---tillery-bloomfield-2022",
    "href": "experience report.html#phase-4-rhetorical-constellating-on-k30---tillery-bloomfield-2022",
    "title": "Triangulating Resource Conservation Movements in Digital Spaces: Topic Modeling and Rhetorical Constellations on r/Anticonsumption",
    "section": "Phase 4: Rhetorical Constellating on K30 - Tillery & Bloomfield (2022)",
    "text": "Phase 4: Rhetorical Constellating on K30 - Tillery & Bloomfield (2022)\n\nPhase 4 Inputs\nIdentifying Nodes, or “Prominences”\nMy first step in engaging with STM results on a deeper level was identifying what type of phenomenon to look for. Reviewing Tillery and Bloomfield’s methods clarified that the point of their analyses were discerning “prominence”: “discursive frequency, but, in another way, prominence can be understood as the driving force and logics undergirding discursive formations and patterns.” (p. 357). Using this definition I searched my CSV file for individual terms which frequently appeared as both Highest Prob and FREX terms in K30 topics. I performed a close read of the surrounding text lines and noted the common language practices, themes of contemplation or questioning, threads of concern, and common objects, people, and relationship mentioned in those surrounding lines of text.\nCreating a Constellation\nFinding connections between different prominences required (1) identifying terms with the most authority in each prominence; (2) defining those authoritative terms based on their textual context; and (3) forming a “prominence identity” which addressed the contextualized meaning of each collection of authoritative terms. Mapping these rhetorical constellations required background knowledge on contemporary concerns related to climate change response, and was significantly informed by my knowledge of resource conservation movements related to anti-consumption such as anti-consumerism, minimalism, self-sustainability, simple living, slow living, slow food, and anti-capitalism.\n\n\nPhase 4 Outputs\n\nThe Prominences\nUs versus them. Signaled by topic term “people.” I also searched my CSV for “everyone,” “family,” “friends,” “coworkers,” “parents,” “folks,” and “folx.”\nThe posts including these terms identified a group of people ranging from broad (“people”) to specific (“parents at my kids’ school”), and described a relationship between said group and the individual user. Most often, users were expressing frustration, exasperation, and confusion about others’ actions and how the user should respond to them. In some instances, the relationships were remarkably complex and large scale. One user expresses frustration at “progressive people” who accept the notion that “nobody is allowed to question the desirability of growth,” and they solicit opinions on this “fundamental growth imperative” central to consumerist culture. In other instances, the relationships are simple yet highly impactful, such as the tension (animosity, even) between folx who use rechargeable vape pens with cartridges versus single-use self-contained apparatuses.\nHome as site of choice making. Signaled by topic terms indicating home and personal-domain-related nouns or verbs, such as “house,” “garden,” “clothes,” and “gifts.”\nPosts which included these terms were most often descriptions of habit-changing and seeking advice on how to alter one’s consumption practices in and around the home, and complaints about others’ use of disposable goods in their everyday life. Users asked for advice on greening their yards and gardens, making their home more energy efficient, reducing food waste, refurbishing different areas of the home, and making personal products last longer.\nChildcare. Signaled by topic term “kids.” I also searched for “child-.”\nThis prominence initially began as part of “Home as site of choice making,” but after seeing how extensively users covered this topic, became one of its own. Posts concerning childcare were often lengthy, and described a variety of concerns about others’ children and the user’s own. Many users expressed fear that kids have no control over learning patterns of overconsumption, and a sense of helplessness when it came to imagining the environments (physical and social) their kids would inherit and co-create. Some posts also involved discussion of how to teach one’s kids the meaning of sustainability and how to demonstrate it as a parent. Many posts included notes on clothing repair, cooking meals, reusing holiday and birthday decorations, alternative transportation (e.g., biking or walking), using cloth napkins, planting rain gardens, and involving kids in taking care of the home.\nConservation and avoidance of particular materials. Signaled by topic terms “plastic,” “gas,” “paper,” “soil,” and “food.”\nPosts mentioning a specific resource almost always focused entirely on the user reducing their consumption of that material whether in effort to conserve it (as in the case of “food,” e.g.) or not contribute to its production (as with “plastic,” e.g.). Where “food” is mentioned, users describe ways to track household consumption and waste, consuming less meat toward water conservation, feeding pets, and buying food items wrapped up in less plastic. On that note, where “plastic” is mentioned, the surrounding text is directly related to eliminating the user’s dependency on plastic. Users note thrifting cloth tote bags, reusing Halloween decorations, finding replacements for Ziplock bags, buying plastic-free makeup, or convincing their barista to use their travel mug instead of a disposable coffee cup.\nThe industrial antagonist. Signaled by topic terms describing dominating cultural structures such as “companies,” “consumption,” “economics,” “christmas,” and “expensive.” I also searched for “holiday,” “tech,” and “politic-.”\nOverall, posts including these terms were criticizing corporate culture, choices make by corporations and governments, the allure of delivery services, online megastores, big brands, and traditions such as holidays. Many users describe feeling inundated with pressure to overtly celebrate holidays with material goods, pressure to use Amazon Prime, to keep their digital devices up to date, and to place anticonsumption at the forefront of their political identity."
  },
  {
    "objectID": "experience report.html#conclusion",
    "href": "experience report.html#conclusion",
    "title": "Triangulating Resource Conservation Movements in Digital Spaces: Topic Modeling and Rhetorical Constellations on r/Anticonsumption",
    "section": "Conclusion",
    "text": "Conclusion\nOverall, r/Anticonsumption is a community whose constituents position themselves in opposition to the industrialized world around them, often with undertones of moral discord. Within the community, they often seek and share advice about altering home space, home practices, and daily habits toward a generally decreased consumption of goods. The partial constellation I was able to produce from the data yielded five prominences or nodes. Each prominence represents a collection of commonplace understandings and beliefs about one’s relationship to Others, including one’s material surroundings and resources as well as the immaterial yet perceivable environment including politics and emotions."
  },
  {
    "objectID": "experience report.html#reflection",
    "href": "experience report.html#reflection",
    "title": "Triangulating Resource Conservation Movements in Digital Spaces: Topic Modeling and Rhetorical Constellations on r/Anticonsumption",
    "section": "Reflection",
    "text": "Reflection\n\nCoding Choices\nIf I could do this project again on r/Anticonsumption, I would prefer to perform STM using not only single words, but also concordance and collocates in order to add detail and most likely accuracy to the topics yielded. I would also like to explore user posts that are explicitly linked to a certain topic(s) in order to better analyze them for meaning. Another way to rigorize my findings would be to explore the content of complete comment data per thread. There is rich conversation here, and a lot of it. On this or a different set of texts in a separate project, I also want to try using the stminsights ShinyApp for more variable, nuanced, and dynamic visualizations (of topics and an actual representation of a rhetorical “constellation”).\n\n\nMy Research\nIn the broader context of my personal research interests, this project (1) offers signposts to subjects, methods, and scholars who are interesting to me; (2) has served as a practical foundation for STM and rhetorical constellating from which to build my methodological skills; (3) and acts as a small square of landscape in the gargantuan ecology of climate change discourse communities who coalesce in digital spaces.\n\n\nPersonal Impact\nOn a more personal note, this project meant a lot to me. I have never coded before in my life, and found it extremely challenging to pivot my problem-solving inclinations from “re/craft a path to arrive at a better conclusion, and leave annotations to edit later” to “find the specific error now and seek a workable solution that fits the code I have already run thus far.” This was excruciating and rewarding in equal parts, not in the least because I have gleaned a more proactive and act-now attitude when encountering troubles in other forms of project and research. I have also developed a durability when it comes to encountering “Error”s and “Warning”s—I no longer fear them, and even have a small toolkit of my own that can help me seek solutions.\nUltimately, completing this work and sharing it with my peers while also hearing about their own coding efforts and insights had a remarkable effect on my self-assuredness and centeredness. As a first-semester-EVER graduate student, I often feel insecure about being surrounded by PhD students and post-docs. I tend to avoid speaking up unless in a small group, despite wanting to be involved and having things I want to say. This course was glaringly different than other TC and Rhetoric courses I have taken, and I struggled each week to parse our course texts and make progress with RStudio. Yet, our classroom was a safe space for me to begin dissolving these barriers, and this project was a much appreciated no-excuses (!!) opportunity to step up and demonstrate what I have learned and what I have not."
  },
  {
    "objectID": "experience report.html#references",
    "href": "experience report.html#references",
    "title": "Triangulating Resource Conservation Movements in Digital Spaces: Topic Modeling and Rhetorical Constellations on r/Anticonsumption",
    "section": "References",
    "text": "References\nCard, D. (2023). week07 [Github repository]. Github. https://github.com/danieljcard1/demo_docs/tree/main/demos/week07 \nCard, D. (2023) week10 [Github repository]. Github. https://github.com/danieljcard1/demo_docs/tree/main/demos/week10 \nJames Cook. (2023, April 17). Extracting reddit data with R and the package RedditExtractoR (2023 update) [Video]. YouTube, https://www.youtube.com/watch?v=Snm0Azfi_hc \nLee, M. SW. (2022). Anti-consumption research: A foundational and contemporary overview. Current Opinion in Psychology, 45, https://doi.org/10.1016/j.copsyc.2022.101319 \nMakri, K., Schlegelmilch, B. B., Mai, R., & Dinhof, K. (2020). What we know about anticonsumption: An attempt to nail jelly to the wall. Psychology & Marketing, 37(2), 167-354. https://doi.org/10.1002/mar.21319 \nSilge, J. (2018, January 25). The game is afoot! Topic modeling of sherlock holmes stories. Julia Silge. https://juliasilge.com/blog/sherlock-holmes-stm/ \nSilge, J. (2018, September 8). Training, evaluating, and interpreting topic models. Julia Silge. https://juliasilge.com/blog/evaluating-stm/ \nTillery, D. & Bloomfield, E. F. (2022). Hyperrationality and rhetorical constellations in digital climate change denial: A multi-methodological analysis of the discourse of watts up with that. Technical Communication Quarterly, 13(4), 356-373. https://doi.org/10.1080/10572252.2021.2019317"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "about me",
    "section": "",
    "text": "Maddi Melchert (they/them) is a graduate student in the Department of Writing Studies at the University of Minnesota Twin Cities, studying Rhetoric and Scientific and Technical Communication.\nTheir research methods of interest are discourse analysis, primarily in digital community spaces; rhetorical constellating; ethnographic studies; and RStudio computational methods for beginners, including structured topic modeling and sentiment analysis.\nMaddi’s research subjects of interest are environmental/climate change discourse; democratization of expertise in responding to changing climate and resources; reciprocity studies; queer rhetorics; relational, engaged pedagogy; and slow thinking.\nThey are always sure to carve out some free time for running, ideally on trails; cooking vegan food; playing Dungeons and Dragons; and reading narrative essay, poetry, fantasy, and nonfiction histories of science.\nIf you’re looking for Maddi on any given day, they are probably in their living room corner chair surrounded by house plants, typing away at a novel while Dizzy Gillespie plays on vinyl."
  }
]